{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "#from my_data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "\n",
    "from Unet import *\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from resnet import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "\n",
    "\n",
    "\n",
    "train_index = np.arange(106,110)\n",
    "val_index = np.arange(124,128)\n",
    "test_index = np.arange(132,134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation from https://github.com/kuangliu/torchcv/blob/master/torchcv/utils/box.py\n",
    "# with slight modifications\n",
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        boxes1 (Tensor[N, 4])\n",
    "        boxes2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "\n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    iou = inter / (area1[:, None] + area2 - inter).type(torch.double)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def box_area(boxes):\n",
    "    \"\"\"\n",
    "    Computes the area of a set of bounding boxes, which are specified by its\n",
    "    (x1, y1, x2, y2) coordinates.\n",
    "    Arguments:\n",
    "        boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n",
    "            are expected to be in (x1, y1, x2, y2) format\n",
    "    Returns:\n",
    "        area (Tensor[N]): area for each box\n",
    "    \"\"\"\n",
    "    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "\n",
    "\n",
    "def get_offsets(gt_boxes, ex_boxes):\n",
    "\n",
    "    ex_width = ex_boxes[:, 2] - ex_boxes[:, 0]\n",
    "    ex_height = ex_boxes[:, 3] - ex_boxes[:, 1]\n",
    "    ex_center_x = ex_boxes[:, 0] + 0.5*ex_width\n",
    "    ex_center_y = ex_boxes[:, 1] + 0.5*ex_height\n",
    "\n",
    "    gt_width = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
    "    gt_height = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
    "    gt_center_x = gt_boxes[:, 0] + 0.5*gt_width\n",
    "    gt_center_y = gt_boxes[:, 1] + 0.5*gt_height\n",
    "\n",
    "\n",
    "    delta_x = (gt_center_x - ex_center_x) / ex_width\n",
    "    delta_y = (gt_center_y - ex_center_y) / ex_height\n",
    "    delta_scaleX = torch.log(gt_width / ex_width)\n",
    "    delta_scaleY = torch.log(gt_height / ex_height)\n",
    "\n",
    "    offsets = torch.cat([delta_x.unsqueeze(0), \n",
    "                    delta_y.unsqueeze(0),\n",
    "                    delta_scaleX.unsqueeze(0),\n",
    "                    delta_scaleY.unsqueeze(0)],\n",
    "                dim=0)\n",
    "    return offsets.permute(1,0)\n",
    "\n",
    "def get_bbox_gt(bboxes1, classes, gt_boxes, sz, device='cpu'):\n",
    "    \n",
    "    bboxes = bboxes1.clone()\n",
    "    bboxes *= 10\n",
    "    bboxes = bboxes + 400\n",
    "    classes += 1\n",
    "    high_threshold = 0.7\n",
    "    low_threshold = 0.3\n",
    "    ex1 = bboxes[:, 0, 3].unsqueeze(0)\n",
    "    ey1 = bboxes[:, 1, 3].unsqueeze(0)\n",
    "    ex2 = bboxes[:, 0, 0].unsqueeze(0)\n",
    "    ey2 = bboxes[:, 1, 0].unsqueeze(0)\n",
    "    ex_boxes = torch.cat([ex1, ey1, ex2, ey2], dim=0)\n",
    "    ex_boxes = ex_boxes.permute(1,0)\n",
    "\n",
    "    ex_width = ex_boxes[:, 2] - ex_boxes[:, 0]\n",
    "    ex_height = ex_boxes[:, 3] - ex_boxes[:, 1]\n",
    "    ex_center_x = ex_boxes[:, 0] + 0.5*ex_width\n",
    "    ex_center_y = ex_boxes[:, 1] + 0.5*ex_height\n",
    "    \n",
    "    gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
    "    gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
    "    gt_center_x = gt_boxes[:, 0] + 0.5*gt_widths\n",
    "    gt_center_y = gt_boxes[:, 1] + 0.5*gt_heights\n",
    "    \n",
    "    ious = box_iou(gt_boxes, ex_boxes)\n",
    "    # ious = ious.permute(1,0)\n",
    "    vals, inds = torch.max(ious, dim=1)\n",
    "    gt_classes = torch.zeros((sz*sz*4)).type(torch.long).to(device)\n",
    "    gt_offsets = torch.zeros((sz*sz*4, 4)).type(torch.double).to(device)\n",
    "    \n",
    "    # HEATMAP CODE\n",
    "    # gt_scores = gt_classes.clone()\n",
    "    # gt_scores[vals > 0.001] = 1\n",
    "    # gt_heat_map_x = gt_center_x[gt_scores == 1]\n",
    "    # gt_heat_map_y = gt_center_y[gt_scores == 1]\n",
    "    \n",
    "    # plotMap(gt_heat_map_x, gt_heat_map_y)\n",
    "    \n",
    "    gt_classes[vals > high_threshold] = classes[inds[vals > high_threshold]] # foreground anchors\n",
    "    gt_classes[vals < low_threshold] = 0 # background anchors\n",
    "    gt_classes[(vals >= low_threshold) & (vals < high_threshold)] = -1 # anchors to ignore\n",
    "    \n",
    "    actual_boxes = ex_boxes[inds[vals > high_threshold]]\n",
    "    ref_boxes = gt_boxes[vals > high_threshold]\n",
    "    g_offsets = get_offsets(ref_boxes, actual_boxes)\n",
    "    gt_offsets[vals > high_threshold] = g_offsets\n",
    "    \n",
    "    return gt_classes, gt_offsets\n",
    "\n",
    "\n",
    "\n",
    "def get_gt_boxes():\n",
    "    '''\n",
    "    Return a matrix with size 4 x 2560000\n",
    "    2560000 = 800*800*4 -> since target BEV map of size 800*800, with 4 different scales\n",
    "    '''\n",
    "    scaleX = [100, 70, 50, 20]\n",
    "    scaleY = [25, 20, 15, 5]\n",
    "    map_sz = 800\n",
    "    widths = torch.tensor(scaleX)\n",
    "    heights = torch.tensor(scaleY)\n",
    "    ref_boxes = []\n",
    "    for x in range(map_sz):\n",
    "        for y in range(map_sz):\n",
    "            x_r = widths + x\n",
    "            y_r = heights + y\n",
    "            x_l = torch.tensor([x, x, x, x])\n",
    "            y_l = torch.tensor([y, y, y, y])\n",
    "            x_r = x_r.unsqueeze(0)\n",
    "            y_r = y_r.unsqueeze(0)\n",
    "            x_l = x_l.unsqueeze(0)\n",
    "            y_l = y_l.unsqueeze(0)\n",
    "            ref_box = torch.cat((x_l, y_l, x_r, y_r))\n",
    "            ref_box = ref_box.permute((1,0))\n",
    "            ref_boxes.append(ref_box)\n",
    "\n",
    "    gt_boxes = torch.stack(ref_boxes).view(-1,4).type(torch.double)\n",
    "    return gt_boxes\n",
    "\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    images = []\n",
    "    gt_boxes = get_gt_boxes()\n",
    "    img_h = 256\n",
    "    img_w = 306\n",
    "    map_sz = 800\n",
    "    class_target = []\n",
    "    box_target = []\n",
    "    for x in batch:\n",
    "        \n",
    "        # stack images, torch.Size([BatchSize, 6*3, 256, 306])\n",
    "        images.append(x[0])\n",
    "        \n",
    "        gt_classes, gt_offsets = get_bbox_gt(x[1]['bounding_box'], x[1]['category'], gt_boxes, map_sz)\n",
    "        class_target.append(gt_classes)\n",
    "        box_target.append(gt_offsets)\n",
    "\n",
    "    samples = torch.stack(images)\n",
    "    samples = samples.view(len(batch), -1, img_h, img_w).double()\n",
    "    class_target = torch.stack(class_target)\n",
    "    box_target = torch.stack(box_target)\n",
    "    return samples, class_target, box_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x12cea0ef0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x12cea0ef0>\n",
      "AssertionError: can only join a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=my_collate_fn)\n",
    "sample, class_target, box_target = iter(trainloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 18, 256, 306])\n",
      "torch.Size([2, 2560000])\n",
      "torch.Size([2, 2560000, 4])\n"
     ]
    }
   ],
   "source": [
    "print(sample.shape)\n",
    "print(class_target.shape)\n",
    "print(box_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoundingBox(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = resnet18()\n",
    "        self.classifier = nn.Conv2d(512, 10, kernel_size=3, padding=1, bias=False)\n",
    "        self.input_shape = (800,800)\n",
    "\n",
    "        self.regressor = nn.Conv2d(10, 4*4, kernel_size=3, padding=1, bias=False)\n",
    "        self.pred = nn.Conv2d(10, 4*9, kernel_size=3, padding=1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        x = F.interpolate(x, size=self.input_shape, mode='bilinear', align_corners=False)\n",
    "\n",
    "        pred_x = self.pred(x)\n",
    "        box_x = self.regressor(x)\n",
    "        return pred_x, box_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(out_pred, class_targets, val=False):\n",
    "        # Sample equal number of positive and negative anchors\n",
    "        out_pred = out_pred.permute((0,2,1))\n",
    "\n",
    "        if val == True:\n",
    "            good_targets = class_targets[class_targets != -1]\n",
    "            good_preds = out_pred[class_targets != -1]\n",
    "            if good_preds.shape[0] == 0:\n",
    "                return torch.tensor(0)\n",
    "            return F.cross_entropy(good_preds, good_targets)\n",
    "\n",
    "        bad_examples = class_targets[class_targets == -1]\n",
    "        foreground_examples = class_targets[class_targets > 0]\n",
    "        foreground_preds = out_pred[class_targets > 0]\n",
    "        background_examples = class_targets[class_targets == 0]\n",
    "        background_preds = out_pred[class_targets == 0]\n",
    "\n",
    "        num_pos = foreground_examples.shape[0]\n",
    "\n",
    "        if num_pos == 0:\n",
    "            print('No positive anchors found in this image !!')\n",
    "            return torch.tensor(0)\n",
    "\n",
    "        perm1 = torch.randperm(background_examples.shape[0], device='cpu')[:]\n",
    "        background_examples = background_examples[perm1]\n",
    "        background_preds = background_preds[perm1]\n",
    "\n",
    "        targets = torch.cat((background_examples, foreground_examples), dim=0)\n",
    "        preds = torch.cat((background_preds, foreground_preds), dim=0)\n",
    "\n",
    "        #loss = focal_loss(preds, targets)\n",
    "        #return loss\n",
    "        return F.cross_entropy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "epochs = 10\n",
    "batch_sz = 2\n",
    "\n",
    "best_val_loss = 100\n",
    "model = BoundingBox().to(device)\n",
    "param_list = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(param_list, lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=False, num_workers=2, collate_fn=my_collate_fn)\n",
    "\n",
    "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=val_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=2, shuffle=False, num_workers=2, collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x12cea0ef0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x12cea0ef0>\n",
      "    self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    w.join()\n",
      "    self._shutdown_workers()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    w.join()\n",
      "AssertionError: can only join a child process\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/504 (0%)]\tLoss: 1.606911\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for i, (samples, class_target, box_target) in enumerate(trainloader):\n",
    "        \n",
    "        samples = sample.to(device)\n",
    "        \n",
    "        out_pred, out_bbox = model(samples)\n",
    "        out_bbox = out_bbox.view(batch_sz, -1, 4)\n",
    "        out_pred = out_pred.view(batch_sz, 9, -1)\n",
    "                \n",
    "        loss = compute_loss(out_pred, class_target)\n",
    "        \n",
    "        if loss.item() != 0:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #self.scheduler.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * len(sample), len(trainloader.dataset),\n",
    "                10. * i / len(trainloader), loss.item()))\n",
    "            \n",
    "    print(\"\\nAverage Train Epoch Loss: \", np.mean(train_losses))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    \n",
    "    for i, (samples, class_target, box_target) in enumerate(valloader):\n",
    "        #model.eval()\n",
    "        samples = sample.to(device).double()\n",
    "        #class_target = class_target.to(device)\n",
    "        #box_target = box_target.to(device)\n",
    "        out_pred, out_bbox = model(samples.double())\n",
    "        out_bbox = out_bbox.view(batch_sz, -1, 4)\n",
    "        out_pred = out_pred.view(batch_sz, 9, -1)\n",
    "        \n",
    "        loss = compute_loss(out_pred, class_targets, val=True)\n",
    "        \n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('Val Epoch: {} [{}/{} ({:.0f}%)]\\tAverage Loss So Far: {:.6f}'.format(\n",
    "                epoch, i * len(sample), len(valloader.dataset),\n",
    "                5. * i / len(valloader), np.mean(val_losses)))\n",
    "            \n",
    "    print(\"Average Validation Epoch Loss: \", np.mean(val_losses))\n",
    "#     global best_val_loss\n",
    "#     if np.mean(val_losses) < best_val_loss:\n",
    "#         best_val_loss = np.mean(val_losses)\n",
    "#         torch.save(model.state_dict(), 'best_val_loss_counting_simple.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

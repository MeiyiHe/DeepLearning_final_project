{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "ResNetModel_BBOX.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZfLzgoo5nMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
        "matplotlib.rcParams['figure.dpi'] = 200\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "from data_helper import UnlabeledDataset, LabeledDataset\n",
        "#from my_data_helper import UnlabeledDataset, LabeledDataset\n",
        "from helper import collate_fn, draw_box\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "from resnet import resnet18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm8V3Q8w5r9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "58f5f50b-61fa-470f-aa53-dade5a4cf248"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5jC0Ils_je8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvKi7Je25nMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# image_folder = '../data'\n",
        "# annotation_csv = '../data/annotation.csv'\n",
        "\n",
        "image_folder = '/content/drive/My Drive/student_data/data'\n",
        "annotation_csv = '/content/drive/My Drive/student_data/data/annotation.csv'\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0);\n",
        "\n",
        "# You shouldn't change the unlabeled_scene_index\n",
        "# The first 106 scenes are unlabeled\n",
        "unlabeled_scene_index = np.arange(106)\n",
        "# The scenes from 106 - 133 are labeled\n",
        "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
        "labeled_scene_index = np.arange(106, 134)\n",
        "\n",
        "\n",
        "train_index = np.arange(106,124)\n",
        "val_index = np.arange(124,134)\n",
        "test_index = np.arange(132,134)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4isRTZha9nRm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "474dc290-4f36-4015-b1c2-9f7da707701b"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmJbFsrl5nMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementation from https://github.com/kuangliu/torchcv/blob/master/torchcv/utils/box.py\n",
        "# with slight modifications\n",
        "def box_iou(boxes1, boxes2):\n",
        "    \"\"\"\n",
        "    Return intersection-over-union (Jaccard index) of boxes.\n",
        "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
        "    Arguments:\n",
        "        boxes1 (Tensor[N, 4])\n",
        "        boxes2 (Tensor[M, 4])\n",
        "    Returns:\n",
        "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
        "            IoU values for every element in boxes1 and boxes2\n",
        "    \"\"\"\n",
        "    area1 = box_area(boxes1)\n",
        "    area2 = box_area(boxes2)\n",
        "\n",
        "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "    iou = inter / (area1[:, None] + area2 - inter).type(torch.double)\n",
        "    return iou\n",
        "\n",
        "\n",
        "def box_area(boxes):\n",
        "    \"\"\"\n",
        "    Computes the area of a set of bounding boxes, which are specified by its\n",
        "    (x1, y1, x2, y2) coordinates.\n",
        "    Arguments:\n",
        "        boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n",
        "            are expected to be in (x1, y1, x2, y2) format\n",
        "    Returns:\n",
        "        area (Tensor[N]): area for each box\n",
        "    \"\"\"\n",
        "    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "\n",
        "\n",
        "def get_offsets(gt_boxes, ex_boxes):\n",
        "\n",
        "    ex_width = ex_boxes[:, 2] - ex_boxes[:, 0]\n",
        "    ex_height = ex_boxes[:, 3] - ex_boxes[:, 1]\n",
        "    ex_center_x = ex_boxes[:, 0] + 0.5*ex_width\n",
        "    ex_center_y = ex_boxes[:, 1] + 0.5*ex_height\n",
        "\n",
        "    gt_width = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
        "    gt_height = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
        "    gt_center_x = gt_boxes[:, 0] + 0.5*gt_width\n",
        "    gt_center_y = gt_boxes[:, 1] + 0.5*gt_height\n",
        "\n",
        "\n",
        "    delta_x = (gt_center_x - ex_center_x) / ex_width\n",
        "    delta_y = (gt_center_y - ex_center_y) / ex_height\n",
        "    delta_scaleX = torch.log(gt_width / ex_width)\n",
        "    delta_scaleY = torch.log(gt_height / ex_height)\n",
        "\n",
        "    offsets = torch.cat([delta_x.unsqueeze(0), \n",
        "                    delta_y.unsqueeze(0),\n",
        "                    delta_scaleX.unsqueeze(0),\n",
        "                    delta_scaleY.unsqueeze(0)],\n",
        "                dim=0)\n",
        "    return offsets.permute(1,0)\n",
        "\n",
        "def get_bbox_gt(bboxes1, classes, gt_boxes, sz):\n",
        "  \n",
        "  bboxes = bboxes1.clone()\n",
        "  bboxes *= 10\n",
        "  bboxes = bboxes + 400\n",
        "  classes += 1\n",
        "  high_threshold = 0.7\n",
        "  low_threshold = 0.3\n",
        "  ex1 = bboxes[:, 0, 3].unsqueeze(0)\n",
        "  ey1 = bboxes[:, 1, 3].unsqueeze(0)\n",
        "  ex2 = bboxes[:, 0, 0].unsqueeze(0)\n",
        "  ey2 = bboxes[:, 1, 0].unsqueeze(0)\n",
        "  ex_boxes = torch.cat([ex1, ey1, ex2, ey2], dim=0)\n",
        "  ex_boxes = ex_boxes.permute(1,0)\n",
        "\n",
        "  ex_width = ex_boxes[:, 2] - ex_boxes[:, 0]\n",
        "  ex_height = ex_boxes[:, 3] - ex_boxes[:, 1]\n",
        "  ex_center_x = ex_boxes[:, 0] + 0.5*ex_width\n",
        "  ex_center_y = ex_boxes[:, 1] + 0.5*ex_height\n",
        "\n",
        "  gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
        "  gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
        "  gt_center_x = gt_boxes[:, 0] + 0.5*gt_widths\n",
        "  gt_center_y = gt_boxes[:, 1] + 0.5*gt_heights\n",
        "\n",
        "  ious = box_iou(gt_boxes, ex_boxes)\n",
        "  # ious = ious.permute(1,0)\n",
        "  vals, inds = torch.max(ious, dim=1)\n",
        "  gt_classes = torch.zeros((sz*sz*4)).type(torch.long)\n",
        "  gt_offsets = torch.zeros((sz*sz*4, 4)).type(torch.double)\n",
        "\n",
        "  # HEATMAP CODE\n",
        "  # gt_scores = gt_classes.clone()\n",
        "  # gt_scores[vals > 0.001] = 1\n",
        "  # gt_heat_map_x = gt_center_x[gt_scores == 1]\n",
        "  # gt_heat_map_y = gt_center_y[gt_scores == 1]\n",
        "\n",
        "  # plotMap(gt_heat_map_x, gt_heat_map_y)\n",
        "\n",
        "  gt_classes[vals > high_threshold] = classes[inds[vals > high_threshold]] # foreground anchors\n",
        "  gt_classes[vals < low_threshold] = 0 # background anchors\n",
        "  gt_classes[(vals >= low_threshold) & (vals < high_threshold)] = -1 # anchors to ignore\n",
        "\n",
        "  actual_boxes = ex_boxes[inds[vals > high_threshold]]\n",
        "  ref_boxes = gt_boxes[vals > high_threshold]\n",
        "  g_offsets = get_offsets(ref_boxes, actual_boxes)\n",
        "  gt_offsets[vals > high_threshold] = g_offsets\n",
        "\n",
        "  return gt_classes, gt_offsets\n",
        "\n",
        "\n",
        "\n",
        "def get_gt_boxes():\n",
        "    '''\n",
        "    Return a matrix with size 4 x 2560000\n",
        "    2560000 = 800*800*4 -> since target BEV map of size 800*800, with 4 different scales\n",
        "    '''\n",
        "    scaleX = [100, 70, 50, 20]\n",
        "    scaleY = [25, 20, 15, 5]\n",
        "    map_sz = 800\n",
        "    widths = torch.tensor(scaleX)\n",
        "    heights = torch.tensor(scaleY)\n",
        "    ref_boxes = []\n",
        "    for x in range(map_sz):\n",
        "        for y in range(map_sz):\n",
        "            x_r = widths + x\n",
        "            y_r = heights + y\n",
        "            x_l = torch.tensor([x, x, x, x])\n",
        "            y_l = torch.tensor([y, y, y, y])\n",
        "            x_r = x_r.unsqueeze(0)\n",
        "            y_r = y_r.unsqueeze(0)\n",
        "            x_l = x_l.unsqueeze(0)\n",
        "            y_l = y_l.unsqueeze(0)\n",
        "            ref_box = torch.cat((x_l, y_l, x_r, y_r))\n",
        "            ref_box = ref_box.permute((1,0))\n",
        "            ref_boxes.append(ref_box)\n",
        "\n",
        "    gt_boxes = torch.stack(ref_boxes).view(-1,4).type(torch.double)\n",
        "    return gt_boxes\n",
        "\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "    images = []\n",
        "    gt_boxes = get_gt_boxes()\n",
        "    img_h = 256\n",
        "    img_w = 306\n",
        "    map_sz = 800\n",
        "    class_target = []\n",
        "    box_target = []\n",
        "    for x in batch:\n",
        "        \n",
        "        # stack images, torch.Size([BatchSize, 6*3, 256, 306])\n",
        "        images.append(x[0])\n",
        "        \n",
        "        gt_classes, gt_offsets = get_bbox_gt(x[1]['bounding_box'], x[1]['category'], gt_boxes, map_sz)\n",
        "        class_target.append(gt_classes)\n",
        "        box_target.append(gt_offsets)\n",
        "\n",
        "    samples = torch.stack(images)\n",
        "    samples = samples.view(len(batch), -1, img_h, img_w).double()\n",
        "    class_target = torch.stack(class_target)\n",
        "    box_target = torch.stack(box_target)\n",
        "    return samples, class_target, box_target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NyXnd9a5nM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BoundingBox(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = resnet18()\n",
        "        self.classifier = nn.Conv2d(512, 10, kernel_size=3, padding=1, bias=False)\n",
        "        self.input_shape = (800,800)\n",
        "\n",
        "        self.regressor = nn.Conv2d(10, 4*4, kernel_size=3, padding=1, bias=False)\n",
        "        self.pred = nn.Conv2d(10, 4*9, kernel_size=3, padding=1, bias=False)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.classifier(x)\n",
        "        x = F.interpolate(x, size=self.input_shape, mode='bilinear', align_corners=False)\n",
        "\n",
        "        pred_x = self.pred(x)\n",
        "        box_x = self.regressor(x)\n",
        "        return pred_x, box_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYStXyfC5nM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(out_pred, class_targets, val=False):\n",
        "  '''\n",
        "  This function is to calculate category prediction loss\n",
        "  '''\n",
        "  # Sample equal number of positive and negative anchors\n",
        "  out_pred = out_pred.permute((0,2,1))\n",
        "\n",
        "  if val == True:\n",
        "      good_targets = class_targets[class_targets != -1]\n",
        "      good_preds = out_pred[class_targets != -1]\n",
        "      if good_preds.shape[0] == 0:\n",
        "          return torch.tensor(0)\n",
        "      return F.cross_entropy(good_preds, good_targets)\n",
        "\n",
        "  bad_examples = class_targets[class_targets == -1]\n",
        "  foreground_examples = class_targets[class_targets > 0]\n",
        "  foreground_preds = out_pred[class_targets > 0]\n",
        "  background_examples = class_targets[class_targets == 0]\n",
        "  background_preds = out_pred[class_targets == 0]\n",
        "\n",
        "  num_pos = foreground_examples.shape[0]\n",
        "\n",
        "  if num_pos == 0:\n",
        "      print('No positive anchors found in this image !!')\n",
        "      return torch.tensor(0)\n",
        "\n",
        "  perm1 = torch.randperm(background_examples.shape[0], device=device)[:]\n",
        "  background_examples = background_examples[perm1]\n",
        "  background_preds = background_preds[perm1]\n",
        "\n",
        "  targets = torch.cat((background_examples, foreground_examples), dim=0)\n",
        "  preds = torch.cat((background_preds, foreground_preds), dim=0)\n",
        "\n",
        "  #loss = focal_loss(preds, targets)\n",
        "  #return loss\n",
        "  return F.cross_entropy(preds, targets)\n",
        "\n",
        "\n",
        "\n",
        "def bbox_loss(box_targets, class_targets, out_bbox):\n",
        "  '''\n",
        "  this function is to calculate bbox loss\n",
        "  '''\n",
        "  inds = (class_targets != 0)\n",
        "  box_targets = box_targets[inds]\n",
        "  out_bbox = out_bbox[inds]\n",
        "  loss_bbox = F.smooth_l1_loss(out_bbox.double(), box_targets.double())\n",
        "  return loss_bbox        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xkthr5o5nM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "batch_sz = 2\n",
        "\n",
        "best_val_loss = 100\n",
        "model = BoundingBox().to(device)\n",
        "param_list = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(param_list, lr=1e-4)\n",
        "\n",
        "\n",
        "transform = torchvision.transforms.ToTensor()\n",
        "\n",
        "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
        "                                  annotation_file=annotation_csv,\n",
        "                                  scene_index=train_index,\n",
        "                                  transform=transform,\n",
        "                                  extra_info=True\n",
        "                                 )\n",
        "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=False, num_workers=2, collate_fn=my_collate_fn)\n",
        "\n",
        "labeled_valset = LabeledDataset(image_folder=image_folder,\n",
        "                                  annotation_file=annotation_csv,\n",
        "                                  scene_index=val_index,\n",
        "                                  transform=transform,\n",
        "                                  extra_info=True\n",
        "                                 )\n",
        "valloader = torch.utils.data.DataLoader(labeled_valset, batch_size=2, shuffle=False, num_workers=2, collate_fn=my_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW8EM_Nk5nM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c7214e0-4cbd-44f0-e790-46e2f0986966"
      },
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    \n",
        "    for i, (samples, class_target, box_target) in enumerate(trainloader):\n",
        "        \n",
        "        #samples = samples\n",
        "        \n",
        "        out_pred, out_bbox = model(samples.float().to(device))\n",
        "        out_bbox = out_bbox.view(batch_sz, -1, 4)\n",
        "        out_pred = out_pred.view(batch_sz, 9, -1)\n",
        "\n",
        "        ## EDIT: calculating bbox loss \n",
        "        #loss = compute_loss(out_pred, class_target.to(device))\n",
        "        loss = bbox_loss(box_target.to(device), class_target.to(device), out_bbox)\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        if loss.item() != 0:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #self.scheduler.step()\n",
        "        \n",
        "        #train_losses.append(loss.item())\n",
        "\n",
        "        #loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, i * len(samples), len(trainloader.dataset),\n",
        "                10. * i / len(trainloader), loss.item()))\n",
        "            \n",
        "    print(\"\\nAverage Train Epoch Loss: \", np.mean(train_losses))\n",
        "    \n",
        "    \n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    \n",
        "    for i, (samples, class_target, box_target) in enumerate(valloader):\n",
        "        #model.eval()\n",
        "        \n",
        "        #class_target = class_target.to(device)\n",
        "        #box_target = box_target.to(device)\n",
        "        out_pred, out_bbox = model(samples.to(device))\n",
        "        out_bbox = out_bbox.view(batch_sz, -1, 4)\n",
        "        out_pred = out_pred.view(batch_sz, 9, -1)\n",
        "        \n",
        "        ## EDIT: calculating bbox loss \n",
        "        #loss = compute_loss(out_pred, class_targets.to(device), val=True)\n",
        "        loss = bbox_loss(box_target.to(device), class_target.to(device), out_bbox)\n",
        "        \n",
        "        val_losses.append(loss.item())\n",
        "        \n",
        "        if i % 10 == 0:\n",
        "            print('Val Epoch: {} [{}/{} ({:.0f}%)]\\tAverage Loss So Far: {:.6f}'.format(\n",
        "                epoch, i * len(samples), len(valloader.dataset),\n",
        "                5. * i / len(valloader), np.mean(val_losses)))\n",
        "            \n",
        "    print(\"Average Validation Epoch Loss: \", np.mean(val_losses))\n",
        "#     global best_val_loss\n",
        "#     if np.mean(val_losses) < best_val_loss:\n",
        "#         best_val_loss = np.mean(val_losses)\n",
        "#         torch.save(model.state_dict(), 'best_val_loss_counting_simple.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/2268 (0%)]\tLoss: 0.082864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vmYaI5N5nNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "\n",
    "\n",
    "unlabeled_scene_index_shuf = unlabeled_scene_index\n",
    "random.shuffle(unlabeled_scene_index_shuf)\n",
    "\n",
    "# split trian/val datasets\n",
    "train_unlabeled_scene_index = unlabeled_scene_index_shuf[:-25]\n",
    "val_unlabeled_scene_index = unlabeled_scene_index_shuf[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(batch):\n",
    "    imgs = []\n",
    "    for x in batch:\n",
    "        front = torch.cat( (torch.tensor( x[0] ), torch.tensor( x[1] ), torch.tensor( x[2] )), 2 )\n",
    "        back = torch.cat( (torch.tensor( x[3] ), torch.tensor( x[4] ), torch.tensor( x[5] )), 2 )\n",
    "        curr_image = torch.cat( (front, back), 1)\n",
    "        #curr_image = torch.cat( (front, back), 1).transpose(2,1).flip(2)\n",
    "        trans = transforms.Compose([transforms.ToPILImage(), \n",
    "                                    transforms.Resize((800,800)),\n",
    "                                    transforms.ToTensor()])\n",
    "        comb = trans(curr_image)\n",
    "        imgs.append(comb)    \n",
    "    return torch.stack(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# train loader\n",
    "unlabeled_trainset = UnlabeledDataset(image_folder=image_folder, \n",
    "                                      scene_index=train_unlabeled_scene_index, \n",
    "                                      first_dim='sample', \n",
    "                                      transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(unlabeled_trainset, \n",
    "                                           batch_size=2,\n",
    "                                           shuffle=True, \n",
    "                                           num_workers=2,\n",
    "                                           collate_fn=my_collate_fn)\n",
    "\n",
    "# val loader\n",
    "unlabeled_valset = UnlabeledDataset(image_folder=image_folder, \n",
    "                                    scene_index=val_unlabeled_scene_index, \n",
    "                                    first_dim='sample', \n",
    "                                    transform=transform)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(unlabeled_valset, \n",
    "                                         batch_size=2, \n",
    "                                         shuffle=True, \n",
    "                                         num_workers=2,\n",
    "                                         collate_fn=my_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# [batch_size, 6, 3, H, W]\n",
    "sample = iter(train_loader).next()\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample[0].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 30\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(800 * 800 *3, d),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, 800 * 800 * 3),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing the VAE\n",
    "\n",
    "epochs = 10\n",
    "# codes = dict(μ=list(), logσ2=list(), y=list())\n",
    "for epoch in range(0, epochs + 1):\n",
    "    # Training\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for i, sample in enumerate(train_loader):\n",
    "\n",
    "        sample = sample.to(device)\n",
    "        sample = sample.view(sample.size(0), -1)\n",
    "        # ===================forward=====================\n",
    "        x_hat = model(sample)\n",
    "\n",
    "        loss = criterion(sample, x_hat)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ===================log========================\n",
    "        if i % 1000  == 0:\n",
    "            avg_loss = float(train_loss / (i+1))\n",
    "            print('Epoch: {} | Avg Loss: {}'.format(epoch, avg_loss))\n",
    "            \n",
    "    avg_loss = float(train_loss / len(train_loader))\n",
    "    print('Trained Epoch {} | Total Avg Loss: {}'.format(epoch, avg_loss))\n",
    "    \n",
    "    # Testing\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for i,sample in enumerate(val_loader):\n",
    "\n",
    "            sample = sample.to(device)\n",
    "            sample = sample.view(sample.size(0), -1)\n",
    "            # ===================forward=====================\n",
    "\n",
    "            x_hat = model(sample)\n",
    "\n",
    "            loss = criterion(sample, x_hat)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # ===================log========================\n",
    "            if i % 1000  == 0:\n",
    "                avg_loss = float(val_loss / (i+1))\n",
    "                print('Epoch: {} | Avg Loss: {}'.format(epoch, avg_loss))\n",
    "            \n",
    "        avg_loss = float(val_loss / len(val_loader))\n",
    "        print('Trained Epoch {} | Total Avg Loss: {}'.format(epoch, avg_loss))\n",
    "    \n",
    "        global best_val_loss\n",
    "        if avg_loss < best_val_loss:\n",
    "            best_val_loss = avg_loss\n",
    "            torch.save(model.state_dict(), 'best_AE_simple.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

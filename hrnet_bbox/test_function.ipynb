{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '../data'\n",
    "annotation_csv = '../data/annotation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_scene_index = np.arange(106, 134)\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=labeled_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=False\n",
    "                                 )\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([2, 6, 3, 256, 306])\n"
    }
   ],
   "source": [
    "sample, target, road_image = iter(trainloader).next()\n",
    "print(torch.stack(sample).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bbox_helperfunc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n"
    }
   ],
   "source": [
    "print(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = bbox_to_label(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(800, 800)\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n4649.0\n"
    }
   ],
   "source": [
    "print(label.shape)\n",
    "print(label)\n",
    "print(label.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bbox = get_bboxes_from_output(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([5, 2, 4])\ntensor([[[ 18.2000,  18.2000,  22.9000,  22.9000],\n         [ -4.3000,  -2.4000,  -4.3000,  -2.4000]],\n\n        [[-23.6000, -23.6000, -19.0000, -19.0000],\n         [ -0.9000,   1.0000,  -0.9000,   1.0000]],\n\n        [[  7.4000,   7.4000,  12.0000,  12.0000],\n         [  2.7000,   4.7000,   2.7000,   4.7000]],\n\n        [[ 34.0000,  34.0000,  39.0000,  39.0000],\n         [  5.8000,   7.9000,   5.8000,   7.9000]],\n\n        [[-12.8000, -12.8000,  -8.0000,  -8.0000],\n         [  6.3000,   8.2000,   6.3000,   8.2000]]], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "print(test_bbox.shape)\n",
    "print(test_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[tensor([[[ 18.2000,  18.2000,  22.9000,  22.9000],\n          [ -4.3000,  -2.4000,  -4.3000,  -2.4000]],\n \n         [[-23.6000, -23.6000, -19.0000, -19.0000],\n          [ -0.9000,   1.0000,  -0.9000,   1.0000]],\n \n         [[  7.4000,   7.4000,  12.0000,  12.0000],\n          [  2.7000,   4.7000,   2.7000,   4.7000]],\n \n         [[ 34.0000,  34.0000,  39.0000,  39.0000],\n          [  5.8000,   7.9000,   5.8000,   7.9000]],\n \n         [[-12.8000, -12.8000,  -8.0000,  -8.0000],\n          [  6.3000,   8.2000,   6.3000,   8.2000]]], dtype=torch.float64),\n tensor([[[-10.6664, -10.6752, -16.2207, -16.2296],\n          [ -2.2139,  -4.3386,  -2.1922,  -4.3169]],\n \n         [[ -3.7310,  -3.7401,  -9.1664,  -9.1754],\n          [ -2.1635,  -4.3332,  -2.1422,  -4.3119]],\n \n         [[  7.2263,   7.2182,   2.3299,   2.3218],\n          [ -2.4688,  -4.4135,  -2.4497,  -4.3944]],\n \n         [[ 15.6763,  15.6526,  11.3561,  11.3324],\n          [  4.4237,   2.5441,   4.4771,   2.5975]],\n \n         [[ 23.9778,  23.9865,  19.2534,  19.2621],\n          [  7.9229,   6.1241,   7.8988,   6.1001]],\n \n         [[ 18.1606,  18.1858,  13.3976,  13.4228],\n          [ -2.0158,  -3.9064,  -2.0803,  -3.9709]],\n \n         [[  4.8248,   4.8334,   0.4104,   0.4189],\n          [  7.9556,   6.1949,   7.9331,   6.1724]],\n \n         [[ 30.6579,  30.7770,  26.2260,  26.3450],\n          [ -2.3285,  -4.1383,  -2.6210,  -4.4308]],\n \n         [[ 23.4510,  23.3874,  28.9518,  28.8882],\n          [ 25.2809,  27.3486,  25.4515,  27.5192]],\n \n         [[-30.1314, -30.1227, -34.8378, -34.8291],\n          [  1.4615,  -0.3322,   1.4375,  -0.3562]],\n \n         [[ 12.0237,  12.0771,   7.6463,   7.6997],\n          [ -2.3897,  -4.1266,  -2.5254,  -4.2623]],\n \n         [[-24.2796, -24.2360, -20.3660, -20.3224],\n          [ 25.7968,  27.8161,  25.7132,  27.7325]],\n \n         [[  1.9768,   1.9693,  -2.8007,  -2.8081],\n          [ -2.3549,  -4.1427,  -2.3363,  -4.1240]],\n \n         [[ 24.8900,  24.9159,  19.2912,  19.3170],\n          [ -2.2030,  -4.1466,  -2.2789,  -4.2224]],\n \n         [[  4.2396,   4.1407,   9.2302,   9.1314],\n          [ 25.0030,  27.0523,  25.2450,  27.2943]],\n \n         [[-32.1747, -32.1119, -37.2837, -37.2209],\n          [ -1.9323,  -3.9741,  -2.0907,  -4.1325]],\n \n         [[-17.9515, -17.9234, -25.4349, -25.4069],\n          [ -2.1197,  -4.2283,  -2.2211,  -4.3296]],\n \n         [[ 27.4793,  27.4727,  26.5704,  26.5639],\n          [ -8.8851,  -9.6760,  -8.8778,  -9.6687]],\n \n         [[-26.4263, -26.3335, -31.2082, -31.1154],\n          [ -2.1501,  -4.0725,  -2.3819,  -4.3044]],\n \n         [[-11.2526, -11.2437,  -6.2502,  -6.2413],\n          [ 25.4526,  27.5943,  25.4331,  27.5748]]], dtype=torch.float64)]"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "new = []\n",
    "new.append(test_bbox)\n",
    "new.append(target[1]['bounding_box'])\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[ 39.0656,  39.0420,  34.0411,  34.0175],\n         [  7.9216,   5.8571,   7.9782,   5.9137]],\n\n        [[-18.9801, -18.9814, -23.5589, -23.5603],\n         [  1.0340,  -0.8957,   1.0364,  -0.8933]],\n\n        [[ 22.9782,  22.9435,  18.3121,  18.2774],\n         [ -2.3560,  -4.2673,  -2.2720,  -4.1834]],\n\n        [[ 12.0548,  12.0257,   7.4764,   7.4473],\n         [  4.7168,   2.7304,   4.7832,   2.7967]],\n\n        [[ -7.9940,  -7.9953, -12.7729, -12.7741],\n         [  8.2002,   6.3935,   8.2027,   6.3960]]], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "print(target[0]['bounding_box'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(0.9503)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "from helper import compute_ats_bounding_boxes\n",
    "\n",
    "compute_ats_bounding_boxes(test_bbox, target[0]['bounding_box'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bite090039fc95743639b991c3c7d67a783",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}